{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 83.\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./ValueIteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration**\n",
    "\n",
    "Given a **policy**, one finds the associated **values** using the **iterative policy evaluation** algorithm. Given **values**, one can find the associated **policy**.  Iterating policy evaluation and finding a policy until the policy does not change is the **policy iteration** algorithm.\n",
    "\n",
    "**Value iteration** proceeds by interleaving value calculations with policy updates.  Convergence occurs when the values do not change.\n",
    "\n",
    "Note that for this algorithm one does not need an initial policy.\n",
    "\n",
    "**Value Iteration Algorithm**\n",
    "```python\n",
    "initialize values (to zero, or randomly)\n",
    "while not converged:\n",
    "    for each state \n",
    "        for each decision (at each state)\n",
    "             value = max(reward + gamma*value at dest)\n",
    "compute policy from values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header-includes:\n",
    "  - \\usepackage[ruled,vlined,linesnumbered]{algorithm2e}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage[ruled,vlined,linesnumbered]{algorithm2e}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1\n",
    "Just a sample algorithmn\n",
    "\\begin{algorithm}[H]\n",
    "\\DontPrintSemicolon\n",
    "\\SetAlgoLined\n",
    "\\KwResult{Write here the result}\n",
    "\\SetKwInOut{Input}{Input}\\SetKwInOut{Output}{Output}\n",
    "\\Input{Write here the input}\n",
    "\\Output{Write here the output}\n",
    "\\BlankLine\n",
    "\\While{While condition}{\n",
    "    instructions\\;\n",
    "    \\eIf{condition}{\n",
    "        instructions1\\;\n",
    "        instructions2\\;\n",
    "    }{\n",
    "        instructions3\\;\n",
    "    }\n",
    "}\n",
    "\\caption{While loop with If/Else condition}\n",
    "\\end{algorithm} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import create_standard_grid\n",
    "from algorithms import compute_policy_from_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from page 83 of Sutton and Barto, RL 2nd. Ed.\n",
    "def value_iteration(gw, gamma=0.9, epsilon=0.9):\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change_in_value = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                old_value = gw.get_value(state)\n",
    "                new_value = float('-inf')\n",
    "                # valid decisions and rewards at current state\n",
    "                dr = gw.valid_decisions_and_rewards(state)\n",
    "                for action, reward in dr.items():\n",
    "                    reward = gw.get_reward_for_action(state, action)\n",
    "                    value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                    value = reward + gamma*value_at_dest\n",
    "                    if value > new_value:\n",
    "                        new_value = value\n",
    "                    gw.set_value(state, new_value)\n",
    "                biggest_change_in_value = max(biggest_change_in_value,\n",
    "                                                  abs(new_value - old_value))\n",
    "        if biggest_change_in_value < epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = create_standard_grid()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute values\n",
    "value_iteration(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Values after Value Iteration\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute policy from values\n",
    "policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"\") \n",
    "print(\"New Policy\")\n",
    "gw.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
